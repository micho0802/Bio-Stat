---
title: "Regression analysis and diagnostics for Albuquerque housing prices"
author: "Michael Dang"
format: 
  html:
    embed-resources: true
date: 2024-09-29
---

This program reads data on housing prices in Albuquerque, New Mexico in 1993. Find more information in the [data dictionary][dd].

[dd]: https://github.com/pmean/datasets/blob/master/albuquerque-housing.yaml

This code is placed in the public domain.

## Load the tidyverse library

For most of your programs, you should load the tidyverse library. The broom package provides a nice way to compute residuals and predicted values. The messages and warnings are suppressed.

```{r setup}
#| message: false
#| warning: false
library(broom)
library(tidyverse)
```

## Read the data and view a brief summary

Use the read_csv function to read the data. The glimpse function will produce a brief summary.

```{r read}
alb <- read_csv(
  file="../data/albuquerque-housing.csv",
  col_names=TRUE,
  col_types="nnnnccc",
  na=".")
glimpse(alb)
```

## m1: regression analysis using features to predict price

You might expect that a house with more features would have a higher sales price. Your first steps are to compute simple descriptive statistics for both the independent variable (features) and the dependent variable (price). Then you should plot the data.

## m1: Calculate descriptive statistics for number of features

```{r features-means}
alb |>
  summarise(
    features_mn=mean(features, na.rm=TRUE),
    features_sd=sd(features, na.rm=TRUE),
    features_min=min(features, na.rm=TRUE),
    features_max=max(features, na.rm=TRUE),
    n_missing=sum(is.na(features)))
```

The average number of features is small (3.5) and the standard deviation (1.4) indicates very little variation. At least one house has zero features and no house has all 13 features.

## m1: Calculate descriptive statistics for price

```{r price-means}
alb |>
  summarize(
    price_mn=mean(price, na.rm=TRUE),
    price_sd=sd(price, na.rm=TRUE),
    price_min=min(price, na.rm=TRUE),
    price_max=max(price, na.rm=TRUE),
    n_missing=sum(is.na(price)))
```

The average price is low (\$106,000), but the standard deviation (\$38,000) shows a fair amount of variation. Note that a dollar sign in R has special meaning. To get it to print normally, you have to put a backslash in front of it.


## Question 1
Calculate descriptive statistics (mean, standard deviation, minimum, and maximum for sqft. Interpret these numbers

```{r sqft-means-q1}
alb |>
  summarize(
    sqft_mn=mean(sqft, na.rm=TRUE),
    sqft_sd=sd(sqft, na.rm=TRUE),
    sqft_min=min(sqft, na.rm=TRUE),
    sqft_max=max(sqft, na.rm=TRUE),
    n_missing=sum(is.na(sqft)))
```

The average sqft per home in this dataset is 1654, but there is significant variability in home sizes, with some being as small as 837 sqft and others as large as 3750 sqft.

## m1: Plot features versus price

```{r scatterplot-1}
alb |>
  ggplot(aes(features, price)) +
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
	  ggtitle("Plot drawn by Steve Simon on 2023-09-24") +
	  xlab("Number of features") +
	  ylab("Price in dollars")
```

There is a weak positive relationship between the number of features and the price of a house.

## Question 2
Draw a plot with price on the y-axis and sqft on the x-axis. Include a linear regression line, but do not extend it beyond the range of the data. Interpret this plot.

```{r scatterplot-2-q2}
alb |>
  ggplot(aes(sqft, price)) +
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
	  ggtitle("Plot drawn by Michael Dang on 2023-09-29") +
	  xlab("Number of square footage") +
	  ylab("Price in dollars")
```

The linear regression line captures the overall upward trend and the plot shows a positive relationship between square footage and price, meaning larger homes generally have higher prices. 

## m1: Use features to predict price

```{r regression-1}
m1 <- lm(price~features, data=alb)
m1
```

The estimated average sales price for a house with no features is \$66,000. This not an extrapolation beyond the range of the data. The estimated average sales price increases by \$11,000 for each additional feature. This is surprisingly large when you look at what the features are. Perhaps houses with more features are bigger and newer. 

## Question 3
Calculate a linear regression model using sqft to predict price. Interpret the slope and intercept.

```{r regression-2-q3}
m2 <- lm(price~sqft, data=alb)
m2
```

The estimated average sales price for a house with no square footage is \$4,781.93, though this doesn't have practical meaning since no house would have zero square footage. The estimated average sales price increases by \$61.37 for each additional square foot, which seems reasonable given that larger homes typically command higher prices. Other factors, such as location and house features, might also play a role in determining the overall price.


## Skip some of the functions for hypothesis tests and p-values

Normally, you would follow this up with various functions like anova(), confint(), or tidy(). This program skips those steps to focus on the diagnostic plots of the residuals.

## m1: Calculate residuals and predicted values

```{r residuals-1}
r1 <- augment(m1)
glimpse(r1)
```

You could have also used the resid() and predict() functions. No interpretation is needed here, as these numbers are better reviewed using various graphical displays.

## m1: Normal probability plot for residuals

```{r qqplot-1}
qqnorm(r1$.resid)
```

The normal probability plot deviates markedly from a straight line, indicating some possible issues with the normality assumption.

Note that you cannnot use ggtitle, xlab, or ylab with the qqnorm function.

## m1: Histogram for residuals

```{r histogram-1}
r1 |>
  ggplot(aes(.resid)) +
    geom_histogram(
      binwidth=10000,
      color="black",
      fill="white") +
	  ggtitle("Plot drawn by Steve Simon on 2023-09-24") +
	  xlab("Residuals from m1")
```

The histogram reinforces these concerns. It looks like the data is skewed to the right.

## Question 4
Draw a normal probability plot and a histogram for the residuals (.resid). Interpret these plots.

```{r residuals-2-q4}
r2 <- augment(m2)
glimpse(r2)

```

Normal probability plot
```{r qqplot-2-q4}
qqnorm(r2$.resid)
```
The points mostly follow the straight line, which suggests that the residuals are approximately normally distributed, though there might be slight deviations at the tails.


Histogram for the residuals
```{r histogram-2-q4}
r2 |>
  ggplot(aes(.resid)) +
    geom_histogram(
      binwidth=5000,
      color="black",
      fill="white") +
	  ggtitle("Plot drawn by Michael Dang on 2023-09-29") +
	  xlab("Residuals from m2")
```
The shape of the histogram suggests that most residuals are clustered near zero, but there might be some spread, which could indicate slight deviations from normality.


## m1: Plot residuals versus features

```{r residual-scatterplot-1}
r1 |>
  ggplot(aes(features, .resid)) + 
    geom_point() +
	  ggtitle("Plot drawn by Steve Simon on 2023-09-24") +
	  xlab("Number of features") +
	  ylab("Residuals from m1") 
```

This plot is difficult to interpret. There is some evidence of heterogeneity. It looks, perhaps, like houses with more features also tend to exhibit more variation. There is no evidence of non-linearity.

## Question 5
Draw a scatterplot of sqft on the x-axis and the residuals on the y-axis. Is there evidence of non-linearity or heterogeneity?

```{r residual-scatterplot-2-q5}
r2 |>
  ggplot(aes(sqft, .resid)) + 
    geom_point() +
	  ggtitle("Plot drawn by Michael Dang on 2023-09-29") +
	  xlab("Number of square footage") +
	  ylab("Residuals from m2") 
```
For non-linearity, there doesn't appear to be a clear systematic pattern in the residuals that would suggest strong non-linearity. However, some subtle clustering of residuals (near the middle range of square footage) could indicate mild non-linear effects, but it's not a pronounced issue.

For heterogeneity, the spread of the residuals seems to increase slightly as the square footage increases, which suggests some degree of heteroscedasticity. 

## m1: Leverage values

```{r leverage-1}
n <- nrow(r1)
r1 |> filter(.hat > 3*2/n)
```

There are four data points with high leverage. These correspond to the houses with the most and the fewest features.

## Question 6
Display the data (if any) for leverage values greater than 3*2/n. Describe where these leverage values are found relative to the independent and/or dependent variables.

```{r leverage-2-q6}
n <- nrow(r2)
r2 |> filter(.hat > 3*2/n)
```

The leverage points are associated with the higher end of square footage values, which suggests that homes with larger square footage are more influential in the model. These leverage points are associated with homes that have larger square footage values (ranging from approximately 2848 to 3750 sqft), and they also tend to have specific characteristics such as being custom-built or located on a corner lot.

## m1: Studentized deleted residual

```{r studentized-1}
r1 |>
  filter(abs(.std.resid) > 3)
```

Only one house, with only an average number of features (3) but with the highest sales price (\$215,000), might be considered an outlier.

## Question 7
Display the data (if any) for studentized deleted residuals (.std.resid) values greater than 3. Describe where these leverage values are found relative to the independent and/or dependent variables.

```{r studentized-2-q7}
r2 |>
  filter(abs(.std.resid) > 3)
```
The first point has a sqft of 3750, which is on the higher end, suggesting that the model struggles to predict accurately for very large homes.Also, the first home is priced at \$129,500, which is relatively low compared to its large square footage, explaining the large negative residual.

The second point has a sqft of 2116, which is closer to the middle range but still has a large residual, indicating the model's difficulty with this specific case.Also, the second home is priced at \$210,000, and the model has under-predicted its price, leading to a large positive residual.


## m1: Cook's distance

```{r cook-1}
r1 |>
  filter(.cooksd > 1)
```

No houses had a large value for Cook's distance. Even though there are a few high leverage points and one outlier, no single data point has unusually high influence on the predicted values.

## Question 8
Display the data (if any) for Cook's distance (.cooksd) values greater than 1. Describe where these leverage values are found relative to the independent and/or dependent variables.

```{r cook-2-q8}
r2 |>
  filter(.cooksd > 1)
```
This particular data point has a Cook's distance of 2.676273, which is significantly greater than 1. This indicates that this point is highly influential in the regression model.

## m2: Using features to predict log(price)

Because there are some concerns about non-normality and heterogeneity, you might consider using a log transformation for price. In this example, a base 10 logarithm is a reasonable choice.

## m2: scatterplot

```{r scatterplot-2}
alb$log_price <- log10(alb$price)
alb |>
  ggplot(aes(features, log_price)) +
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
	  ggtitle("Plot drawn by Steve Simon on 2023-09-24") +
	  xlab("Number of features") +
	  ylab("Log base 10 of price in dollars")
```

There is a weak positive linear relationship between log price and features.

## Question 9
Calculate the regression equation predicting log10 of price using sqft. Transform the coefficients back to the original scale of measurement and interpret these values.

```{r scatterplot-2-q9}
alb$log_price <- log10(alb$price)
alb |>
  ggplot(aes(sqft, log_price)) +
    geom_point() +
    geom_smooth(method="lm", se=FALSE) +
	  ggtitle("Plot drawn by Michael Dang on 2023-09-29") +
	  xlab("Number of square footage") +
	  ylab("Log base 10 of price in dollars")
```

It look the same from question 2, suggesting that the log transformation did not significantly affect the relationship between square footage and price.

## m2: linear regression on log transformed price

```{r regression-2}
m2 <- lm(log_price~features, data=alb)
m2
```

The estimated average log price is 4.8 for a house with no features. The estimated average log price increases by 0.043 for each additional feature. These numbers are easier to interpret when transformed back to the original scale.

## m2: Coefficients back transformed to original scale

```{r back-transform-2}
10^(coef(m2))
```

The estimated average price is \$71,000 for a house with no features. The estimated average price increases by 1.10 (10%) for each additional feature.

## m2: Normal probability plot

```{r qqplot-2}
r2 <- augment(m2)
qqnorm(r2$.resid)
```

The normal probability plot is close to a straight line, indicating a reasonably close fit to a normal distribution.

## m2: Histogram of residuals

```{r histogram-2}
r2 |>
  ggplot(aes(.resid)) +
    geom_histogram(
      binwidth=0.05,
      color="black",
      fill="white") +
	  ggtitle("Plot drawn by Steve Simon on 2023-09-24") +
	  xlab("Residuals from m2")
```

The histogram of residuals also indicates a close fit to a normal distribution. The regression model using log price does a better job meeting the normality assumption.

## m2: Plot residuals versus features

```{r residual-scatterplot-2}
r2 |>
  ggplot(aes(features, .resid)) + 
    geom_point() +
	  ggtitle("Plot drawn by Steve Simon on 2023-09-24") +
	  xlab("Number of features") +
	  ylab("Residuals from m2")
```

This plot is difficult to interpret. There is certainly no evidence of non-linearity, but perhaps the problems with heterogenity persist even after the log transformation. Houses with zero or one features seem to have less variation than the rest of the data.


## Question 10 
Calculate diagnostic plots (normal probability plot, histogram, and sqft versus residuals). Do these plots show that a model using log10 price better meets the assumptions for linear regression?

Linear regression on log transformed price
```{r regression-2-q10}
m3 <- lm(log_price~sqft, data=alb)
m3
```
The estimated average sales log price for a house with no square footage is 4.6, though this doesn't have practical meaning since no house would have zero square footage. The estimated average sales log price increases by 0.0002 for each additional square foot.

Coefficients back transformed to original scale
```{r back-transform-2-q10}
10^(coef(m3))
```

The estimated average price is \$42,605 for a house with no features. The estimated average price increases by 1.00052 (0.52%) for each additional square footage.

Normal probability plot
```{r qqplot-2-q10}
r3 <- augment(m3)
qqnorm(r3$.resid)
```
The residuals mostly follow the straight line, though there are slight deviations at the extremes, particularly at the lower end. This indicates that while the residuals are fairly close to a normal distribution, there may be slight deviations from normality at the tails.

Histogram of residuals
```{r histogram-2-q10}
r3 |>
  ggplot(aes(.resid)) +
    geom_histogram(
      binwidth=0.05,
      color="black",
      fill="white") +
	  ggtitle("Plot drawn by Michael Dang on 2023-09-29") +
	  xlab("Residuals from m3")
```
The residuals are fairly symmetric and centered around zero, which suggests a near-normal distribution. There are a few outliers at the extreme ends, but overall, the distribution is more centered than in the original (untransformed) model.

Plot residuals versus feature
```{r residual-scatterplot-2-q10}
r3 |>
  ggplot(aes(sqft, .resid)) + 
    geom_point() +
	  ggtitle("Plot drawn by Michael Dang on 2023-09-29") +
	  xlab("Number of square footage") +
	  ylab("Residuals from m2")
```
The residuals appear to be scattered randomly around zero, without a clear pattern or funnel shape. This indicates that heteroscedasticity is not a significant issue, and the variance of the residuals is more consistent compared to the untransformed model.
